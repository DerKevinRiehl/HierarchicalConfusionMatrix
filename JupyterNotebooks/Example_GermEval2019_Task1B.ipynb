{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adequate-spanking",
   "metadata": {},
   "source": [
    "# Example GermEval2019_Task1B (DAG, MPL, NMLNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "together-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2019 Competition on hierarchical classification of texts\n",
    "# Task 1B: (DAG, MPL, NMLNP) classification problem\n",
    "# More infos can be found here: https://2019.konvens.org/germeval\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from hierarchical_confusion_matrix import determineHierarchicalConfusionMatrix, getLeafNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method loads the structure / classification hierarchy for GermEval2019 data from a given file,\n",
    "and returns it as a graph object.\n",
    "\"\"\"\n",
    "def loadHierarchy(file, level=-1):\n",
    "    # Load GermEval2019 Hierarchy\n",
    "    f = open(file, \"r\", encoding=\"utf8\")\n",
    "    edges = []\n",
    "    for l in f.readlines():\n",
    "        edges.append(l.replace(\"\\n\",\"\").split(\"\\t\"))\n",
    "    f.close()\n",
    "    # Determine root nodes\n",
    "    root_nodes = []\n",
    "    for i in range(0,len(edges)):\n",
    "        cat = edges[i][0]\n",
    "        if(cat in root_nodes):\n",
    "            continue\n",
    "        found = False\n",
    "        for j in range(0,len(edges)):\n",
    "            if(cat == edges[j][1] and i != j):\n",
    "                found = True\n",
    "                break\n",
    "        if(not found):\n",
    "            root_nodes.append(cat)\n",
    "    # Add root node connection\n",
    "    if(level==1):\n",
    "        edges = []\n",
    "    for n in root_nodes:\n",
    "        edges.append([\"root\",n])\n",
    "    # Convert to Networkx Graph\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(edges)\n",
    "    return graph\n",
    "\n",
    "\"\"\"\n",
    "This method loads the evaluation data from GermEval2019_Task1B (true labels and prediction labels)\n",
    "\"\"\"\n",
    "def loadEvaluationData_GermEval2019_Task1B(true_label_file, pred_label_file):    # Load True Labels of task A (Tree(1Level), MPL, MLNP)\n",
    "    true_label_data = {}\n",
    "    pred_label_data = {}\n",
    "    eval_label_data = {}\n",
    "    # Load data from true label file\n",
    "    f = open(true_label_file, \"r\", encoding=\"utf8\")\n",
    "    line = f.readline()\n",
    "    while not line.startswith(\"subtask_b\"):\n",
    "        line = f.readline()\n",
    "    line = f.readline()\n",
    "    while line!=\"\":\n",
    "        parts = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        l_list = []\n",
    "        for p in parts[1:]:\n",
    "            if(p!=\"\"):\n",
    "                l_list.append(p)\n",
    "        true_label_data[parts[0]] = l_list\n",
    "        line = f.readline()\n",
    "    f.close()    \n",
    "    # Load data from prediction label file\n",
    "    f = open(pred_label_file, \"r\", encoding=\"utf8\")\n",
    "    line = f.readline()\n",
    "    while not line.startswith(\"subtask_b\"):\n",
    "        line = f.readline()\n",
    "    line = f.readline()\n",
    "    while line!=\"\":\n",
    "        parts = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        l_list = []\n",
    "        for p in parts[1:]:\n",
    "            if(p!=\"\"):\n",
    "                l_list.append(p)\n",
    "        pred_label_data[parts[0]] = l_list\n",
    "        line = f.readline()\n",
    "    f.close()    \n",
    "    # Convert data for hierarchical classification paths\n",
    "    for key in true_label_data:\n",
    "        minim_paths = createMinimPathsFromLabels(graph, true_label_data[key])\n",
    "        true_labels_corrected = []\n",
    "        for p in minim_paths:\n",
    "            true_labels_corrected.append(getLeafNode(p))\n",
    "        true_label_data[key] = true_labels_corrected\n",
    "    # Convert data for hierarchical classification paths\n",
    "    for key in pred_label_data:\n",
    "        minim_paths = createMinimPathsFromLabels(graph, pred_label_data[key])\n",
    "        if(len(minim_paths)==0):\n",
    "            minim_paths = [[\"root\"]]\n",
    "        pred_label_data[key] = minim_paths\n",
    "    # Process evaluation data results\n",
    "    for key in true_label_data:\n",
    "        if(key in pred_label_data):\n",
    "            eval_label_data[key] = {}\n",
    "            eval_label_data[key][\"true\"] = true_label_data[key]\n",
    "            eval_label_data[key][\"pred\"] = pred_label_data[key]\n",
    "    n_nopredictions = 0\n",
    "    for key in true_label_data:\n",
    "        if(key not in pred_label_data):\n",
    "            eval_label_data[key] = {}\n",
    "            eval_label_data[key][\"true\"] = true_label_data[key]\n",
    "            eval_label_data[key][\"pred\"] = [[\"root\"]]\n",
    "            n_nopredictions += 1\n",
    "    # Return results\n",
    "    return eval_label_data, n_nopredictions\n",
    "\n",
    "\"\"\"\n",
    "This method counts the number of nodes from node_list that appear in a path.\n",
    "\"\"\"\n",
    "def countNodesOnPath(path, node_list):\n",
    "    ctr = 0\n",
    "    intersect = []\n",
    "    for node in node_list:\n",
    "        if(node in path):\n",
    "            ctr += 1\n",
    "            intersect.append(node)\n",
    "    return ctr, intersect\n",
    "\n",
    "\"\"\"\n",
    "This method creates the minimum length path to a given node from labels in label_data.\n",
    "\"\"\"\n",
    "def createMinimPathsFromLabels(graph, label_data):\n",
    "    if(len(label_data)==0):\n",
    "        return []\n",
    "    selected_nodes = []\n",
    "    selected_paths = []\n",
    "    remaining_nodes = label_data.copy()\n",
    "    remaining_paths = []\n",
    "    for val in label_data:\n",
    "        for p in nx.all_simple_paths(graph, \"root\", val):\n",
    "            remaining_paths.append(p)\n",
    "    while True:\n",
    "        # Select path that covers most of remaining nodes\n",
    "        max_n = -1\n",
    "        s_nodes = []\n",
    "        s_path  = []\n",
    "        for path in remaining_paths:\n",
    "            ctr, nod = countNodesOnPath(path, remaining_nodes)\n",
    "            if(ctr>max_n):\n",
    "                max_n   = ctr\n",
    "                s_nodes = nod.copy()\n",
    "                s_path  = path.copy()\n",
    "        # Add Path to selected Paths and remove from remaining\n",
    "        selected_paths.append(s_path)\n",
    "        for n in s_nodes:\n",
    "            remaining_nodes.remove(n)\n",
    "            selected_nodes.append(n)\n",
    "        remaining_paths.remove(s_path)\n",
    "        # Break loop if all nodes covered\n",
    "        if(len(remaining_nodes)==0):\n",
    "            break\n",
    "    return selected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dated-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo\tF1\tPPV\tREC\tACC\tMCC\tTP\tTN\tFP\tFN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramFiles\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averbis__BOHB_CNN.txt \t 0.6034221203034045 \t 0.6461654703437855 \t 0.5659827928524156 \t 0.9228716104951147 \t 53922.56770879811 \t 8552 \t 125951 \t 4683 \t 6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramFiles\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comtravo-DS__global_clf_cnn.txt \t 0.5046392466417394 \t 0.5291128212574415 \t 0.4823295830575778 \t 0.8931169975946095 \t nan \t 7288 \t 112270 \t 6486 \t 7822\n",
      "Comtravo-DS__local_clf_logit_cnn.txt \t 0.5598878198885989 \t 0.6803938275111238 \t 0.4756452680344143 \t 0.9133226447371449 \t nan \t 7187 \t 111871 \t 3376 \t 7923\n",
      "DFKI-SLT__full.txt \t 0.5774319066147859 \t 0.7575497044599677 \t 0.4665122435473197 \t 0.9205975387315001 \t 55110.092273545924 \t 7049 \t 112567 \t 2256 \t 8061\n",
      "DFKI-SLT__full2.txt \t 0.5774319066147859 \t 0.7575497044599677 \t 0.4665122435473197 \t 0.9205975387315001 \t 55110.092273545924 \t 7049 \t 112567 \t 2256 \t 8061\n",
      "DFKI-SLT__text-only.txt \t 0.5354993983152828 \t 0.679735234215886 \t 0.44176042356055595 \t 0.9105667196984909 \t 23231.667309130193 \t 6675 \t 111227 \t 3145 \t 8435\n",
      "EricssonResearch__fconv_4LYFP_7EKHC_WNG1A.txt \t 0.44739945508519374 \t 0.5802130576943361 \t 0.3640635340833885 \t 0.876121280630105 \t nan \t 5501 \t 90606 \t 3980 \t 9609\n",
      "EricssonResearch__fconv_A6C1Y.txt \t 0.6338007159904535 \t 0.7259525029899196 \t 0.5624090006618133 \t 0.9287703824058492 \t nan \t 8498 \t 119546 \t 3208 \t 6612\n",
      "EricssonResearch__fconv_F8V17.txt \t 0.6188946975354742 \t 0.7101113967437875 \t 0.5484447385837193 \t 0.9246233382570163 \t nan \t 8287 \t 116907 \t 3383 \t 6823\n",
      "HSHL__LogisticRegression_NaiveBayes1.txt \t 0.5665164809105999 \t 0.7031985871271585 \t 0.4743216412971542 \t 0.9119905635396356 \t nan \t 7167 \t 106488 \t 3025 \t 7943\n",
      "HSHL__LogisticRegression_NaiveBayes2.txt \t 0.5555372852092412 \t 0.7331814236111112 \t 0.4471872931833223 \t 0.91201171875 \t 15385.255694185515 \t 6757 \t 105311 \t 2459 \t 8353\n",
      "knowcup__DL_single_test.txt \t 0.6151461302884798 \t 0.7174353999470853 \t 0.5383851753805426 \t 0.9263895517854819 \t nan \t 8135 \t 119968 \t 3204 \t 6975\n",
      "LT-UHH__baseline.txt \t 0.48765112668767935 \t 0.843175532780218 \t 0.343017868960953 \t 0.9037914524478367 \t nan \t 5183 \t 97128 \t 964 \t 9927\n",
      "LT-UHH__baseline_wo_correction.txt \t 0.48765112668767935 \t 0.843175532780218 \t 0.343017868960953 \t 0.9037914524478367 \t nan \t 5183 \t 97128 \t 964 \t 9927\n",
      "LT-UHH__contender.txt \t 0.5996803991113536 \t 0.7294017256091779 \t 0.5091330244870946 \t 0.9244673888263801 \t 20514.655796465802 \t 7693 \t 118017 \t 2854 \t 7417\n",
      "NoTeam__GRU_Attention_ensemble1.txt \t 0.2749089828367635 \t 0.3133468834688347 \t 0.24487094639311713 \t 0.8017551343774758 \t 8381.23532413504 \t 3700 \t 75236 \t 8108 \t 11410\n",
      "twistbytes__sklearn_hier_threshold.txt \t 0.6320140539423375 \t 0.6590043818691186 \t 0.6071475843812045 \t 0.9291310375937855 \t 41252.36084487199 \t 9174 \t 130886 \t 4747 \t 5936\n",
      "twistbytes__sklearn_hier_threshold_and_roots_baseline_thresholding.txt \t 0.6320140539423375 \t 0.6590043818691186 \t 0.6071475843812045 \t 0.9291310375937855 \t 41252.36084487199 \t 9174 \t 130886 \t 4747 \t 5936\n",
      "twistbytes__thresholding.txt \t 0.393804631191535 \t 0.8650348079946104 \t 0.2549305095962938 \t 0.8826840512039253 \t 8910.24972033738 \t 3852 \t 85375 \t 601 \t 11258\n"
     ]
    }
   ],
   "source": [
    "# Load GermEval2019 Hierarchy\n",
    "path = \"CaseStudies/GermEval2019\"\n",
    "hierarchy_file = os.path.join(path,\"hierarchy.txt\")\n",
    "graph = loadHierarchy(hierarchy_file) \n",
    "\n",
    "# List all available algorithms\n",
    "true_label_file = os.path.join(path,\"blurbs_test_label.txt\")\n",
    "algo_folder = os.listdir(os.path.join(path, \"system-submissions/test-phase-txt\"))\n",
    "\n",
    "# For each algorithm determine hierarchical confusion matrix\n",
    "print(\"algo\\tF1\\tPPV\\tREC\\tACC\\tMCC\\tTP\\tTN\\tFP\\tFN\")\n",
    "for algo in algo_folder:\n",
    "    pred_label_file = os.path.join(path, \"system-submissions/test-phase-txt\", algo)\n",
    "    f = open(pred_label_file, \"r\", encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    if(\"subtask_b\\n\" not in lines):\n",
    "        continue\n",
    "    eval_label_data, nn = loadEvaluationData_GermEval2019_Task1B(true_label_file, pred_label_file)\n",
    "    # Predict Confusion Matrix\n",
    "    h_confusion = {}\n",
    "    h_confusion_total = []\n",
    "    for key in eval_label_data:\n",
    "        h_confusion[key] = determineHierarchicalConfusionMatrix(graph, eval_label_data[key][\"true\"], eval_label_data[key][\"pred\"])\n",
    "        h_confusion_total.append(h_confusion[key])\n",
    "    h_confusion_total = np.sum(np.asarray(h_confusion_total),axis=0)\n",
    "    F1 = 2*h_confusion_total[0]/(2*h_confusion_total[0]+h_confusion_total[2]+h_confusion_total[3])\n",
    "    PPV = h_confusion_total[0]/(h_confusion_total[0]+h_confusion_total[2])\n",
    "    REC = (h_confusion_total[0])/(h_confusion_total[0]+h_confusion_total[3])\n",
    "    ACC = (h_confusion_total[0]+h_confusion_total[1])/(h_confusion_total[0]+h_confusion_total[1]+h_confusion_total[2]+h_confusion_total[3])\n",
    "    MCC = (h_confusion_total[0]*h_confusion_total[1]-h_confusion_total[2]*h_confusion_total[3])/np.sqrt((h_confusion_total[0]+h_confusion_total[2])*(h_confusion_total[0]+h_confusion_total[3])*(h_confusion_total[1]+h_confusion_total[2])*(h_confusion_total[1]+h_confusion_total[3]))\n",
    "    print(algo, \"\\t\", F1, \"\\t\", PPV, \"\\t\", REC, \"\\t\", ACC, \"\\t\", MCC, \"\\t\", h_confusion_total[0], \"\\t\", h_confusion_total[1], \"\\t\", h_confusion_total[2], \"\\t\", h_confusion_total[3])    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
